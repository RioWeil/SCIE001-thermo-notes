\subsubsection{The Arrow of Time}
One interesting property of a lot of physics equations is that they are invariant under time reversal; that is, equations like Newton's second law 
\begin{align*}
\vec{F} = m\vec{a}
\end{align*}
can be applied even in cases where time was running in reverse\footnote{Assuming no friction, of course.}. However, it is clear in our immediate experience that a lot of things are not invariant under time reversal. If I were to show you a video of heat flowing from a cold bath of water to a hot bath of water, you would tell me immediately that the video must be playing in reverse. If the laws of physics are supposed to work the same way in either direction, what's going on here?

As we discussed previously, where we see phenomena that only proceed in one time direction, what we are seeing is something go from a low entropy state to a higher entropy state. After the process, by the second law of thermodynamics, we can't go back to the lower entropy state. But this explanation feels a bit unsatisfactory; sure, we've accepted the second law of thermodynamics as a law, but there's still this underlying feeling of "Well, why \textbf{can't} things go back to a lower entropy state?" The statistical definition will allow us to settle this confusion once and for all.

From a statistical viewpoint, what are we saying when a system goes from a low to a high entropy state? Well by the definition of $S = k_b\ln\mathbb{W}$ for the entropy of a macrostate, what we're really saying is that the system moves from a macrostate with a low number of microstates to a high number of microstates. Okay sure, but why? Well let's think back to the fundamental assumption of statistical mechanics that I just outlined a page ago. This assumption tells us that every microstate is equally as likely to appear, and each microstate persists for the same amount of time. Well, if every microstate is equally as likely, then simple notions of probability will tell us that the system at any given point will tend to a macrostate with more microstates.

Let's try to make this a little more concrete with an example. Say I was keeping tracks of 10 numbered cows in a field, separated into a left and right hand side, where the cows were allowed to walk around the field totally randomly. Then, a macrostate of the system could be defined by how many cows were on the left side of the field (where I have 11 microstates, from 0-10 cows on the left side). Now, suppose I start in a configuration where all 10 cows are on the left side of the field. There are a couple ways in which I could arrange the cows in this scenario. However, the macrostate corresponding to 5 cows on the right side and 5 cows on the left side of the field has a lot more microstates; there are a lot more ways I could arrange 5 and 5 cows on either side of the field compared to having all the cows on one side. With this in mind, let's imagine that the cows start all on one side, I walk away from the field, and come back some amount of time later after the cows were free to move around. Since every configuration of cows is equally likely, I would be most likely see a state where the cows were distributed equally on both sides of the field (as we would expect). Probabilistically, the system has evolved from an initial low entropy state (all cows on one side) to a higher entropy state (cows distributed on both sides).

Now, imagine instead of left and right side of a field, we have two containers of gas in thermal contact, which can freely exchange units of energy (which have replaced the cows in this scenario). Although now we have an extremely large number of energy units instead of just 10\footnote{Many 10s of orders of magnitude more...}, the same sort of logic applies; if I start in a macrostate with all of the energy units on one side (i.e. one gas container is very hot, and the other is not), with time, the system will tend to a macrostate of thermal equilibrium (approximately equal thermal energy on either side) as there are so many more microstates for the equilibrium temperature macrostate compared to a macrostate where one side has a lot more energy units than the other. Hence, we have come up with a statistical explanation for why heat tends to flow from hot objects to cold objects!

Note that this analysis reveals something interesting about the second law of thermodynamics; it's very much a statistical law, that tells us that a isolated system is (extremely) likely to tend to a high entropy state. Note that both in the cows and the heat example, there technically is a possibility that the system could temporarily return to a low entropy state (either with all the cows, or all the thermal energy units on one side); its just that this configuration is \textbf{extremely unlikely}. In fact, as we make our system larger and larger, the likelihood that the system could ever return to a low entropy state becomes more and more unlikely, and therefore in the limit of large/macroscopic systems, we can treat the second law as (basically) absolute. 

This concludes our discussion of Science One Thermodynamics. If you enjoyed this part of the course, I would highly recommend taking PHYS 203 (Thermal Physics) next year, even if you decide not to go into a physics major\footnote{If you do go into physics, you won't have a choice!}. If you go into chemistry, you will also cover more thermodynamics material in the CHEM 205+304 sequence. Schroeder's "An Introduction to Thermal Physics" is a good text to consult for further reading on the subject (if you wanted to go beyond the material covered in your textbooks this year). We hope that even if you found this unit challenging at times, you can appreciate how we can learn so much about the workings of the universe from a couple (fairly simple) definitions and laws; if not, we hope that you take pride in the fact that you've made it through one of the most difficult units of Science One physics!

